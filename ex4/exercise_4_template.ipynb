{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768b51d1",
   "metadata": {},
   "source": [
    "### Below is a concise yet friendly explanation of the attention mechanism for assignment introduction:\n",
    "\n",
    "#### Attention Mechanism (Adapted from ‚ÄúAttention Is All You Need‚Äù)\n",
    "\n",
    "The attention mechanism, introduced in Attention Is All You Need (Vaswani et al., 2017), processes inputs represented as vectors (each row is a token embedding of dimension $ùê∑$). We compute three sets of vectors: queries (Q), keys (K), and values (V). The core steps are:\n",
    "\n",
    "1. **Linear Transformations:**  \n",
    "\n",
    "Let $X$ be the input matrix, where each of the rows corresponds to a token in the input sequence, and each row is a $d$-dimensional embedding vector.\n",
    "\n",
    "To compute attention, we first project $X$ into three different representations using learned weight matrices:\n",
    "\n",
    "Each input vector is transformed into $Q$, $K$, and $V$ using learnable weights.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_i &= X W_i^Q, \\\\\n",
    "K_i &= X W_i^K, \\\\\n",
    "V_i &= X W_i^V.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Each head \\(i\\) has its own learnable parameters $W_i^Q$, $W_i^K$, and $W_i^V$, which transform the input into queries, keys, and values, respectively.\n",
    "\n",
    "\n",
    "2. **Scaled Dot-Product Attention:**  \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V.\n",
    "\\end{equation}\n",
    "\n",
    "Here, $QK^T$ produces a matrix of scores that measures how relevant each ‚Äúquery‚Äù position is to every ‚Äúkey‚Äù position. \n",
    "\n",
    "The softmax function converts these scores into attention weights (non-negative values that sum to 1 across each row).\n",
    "\n",
    "These weights are then used to combine the values ùëâ to produce the final output.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. **Multi-Head Attention:**  \n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where } \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V).\n",
    "$$\n",
    "\n",
    "\n",
    "Multiple attention heads allow the model to attend to different aspects of the input simultaneously. Their outputs are concatenated and linearly transformed to produce the final result.\n",
    "\n",
    "\n",
    "\n",
    "*Note:* In this assignment, you are only required to experiment with the provided $Q$, $K$, and $V$ matrices to perform the matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ccbf8",
   "metadata": {},
   "source": [
    "## Self-attention Computer Assignment \n",
    "\n",
    "\n",
    "Implement the multi-head self-attention operation, taking in a set of $N$ vectors of $D$ dimensions and outputting a matrix of the same size. Do this without relying on neural network libraries, but rather write directly the required operations in NumPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "434b6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6598d929-2af6-4d94-acee-1341af2a1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size\n",
    "N = 5\n",
    "D = 6\n",
    "\n",
    "X = [[ 0.7, -0.8, -1.2,  -1.,  -0., -0.3],\n",
    "     [ 2.7,  0.1,  1.6,  1.8,  1.5,  0.3],\n",
    "     [ 0.1,  2.6, -0.1, -1.3, -0.5, -0.7],\n",
    "     [ 1.1,  1.5,   1., -0.5,  0.4,  0.4],\n",
    "     [-0.7, -0.7,  0.7, -1.5, -0.8,  1. ]]\n",
    "\n",
    "Wq = [[-1.7,  1.6,  0.9, -0.5,  0.4,  -1.],\n",
    "      [-0.4,  1. , -0.3,  1. ,  0.5,  1.1],\n",
    "      [ 0.4, -0.9,  -1.,  0.5, -1.4,  0. ],\n",
    "      [ 0.3,  1.4, -1.2,  0.2,  0.1,  1.6],\n",
    "      [-0.8,  0.8, -0.7, -1.3,  0.3,  0.8],\n",
    "      [ 1.1,  0.3, -1.5, -2.3,  2.2, -0.7]]\n",
    "\n",
    "Wk = [[ 0.3, -0.4, -1.3,  0.3, -1.7,  1.1],\n",
    "      [-2.3, -1.1,  0.6, -1.2,  2.2,  0.3],\n",
    "      [ 1.1, -0.4, -0.5,  1.9, -1.1, -1.2],\n",
    "      [-0.4,  1. , -1.7,  0. , -3.3, -1.4],\n",
    "      [-0.9, -1.1, -1. ,  1.4,  1.3,  1.2],\n",
    "      [-0.7,  0.4,  0.4, -1.4, -0.2, -0.5]]\n",
    "\n",
    "Wv = [[-0.1,  0.7,  1. , -0.1,  1.6,  0.9],\n",
    "      [ 0.4, -1. , -0.7, -0.6, -0.9, -0.1],\n",
    "      [-0.4,  0.5, -1.4,  0.1,  0.6,  0.4],\n",
    "      [ 1.4, -1.3, -1.3, -0.6,  1.6, -0.2],\n",
    "      [-0.4, -0.6, -1.4, -1. ,  0.4, -0.8],\n",
    "      [ 0.2,  0.5,  0.4, -0.5,  1.4,  2.3]]\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "Wq = np.array(Wq)\n",
    "Wk = np.array(Wk)\n",
    "Wv = np.array(Wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b382714-52d9-4a53-9361-19e81d5b64c6",
   "metadata": {},
   "source": [
    "### (a) Implement the self-attention operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0314ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(X, Wq, Wk, Wv):\n",
    "    Q = X @ Wq\n",
    "    K = X @ Wk\n",
    "    V = X @ Wv\n",
    "\n",
    "    d_k = K.shape[1]\n",
    "    scores = Q @ K.T\n",
    "    scores /= np.sqrt(d_k)\n",
    "    exp_scores = np.exp(scores)\n",
    "    attention_weights = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4298990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention Output:\n",
      " [[-0.7 -0.9  0.5  0.1 -5.5 -1.1]\n",
      " [-0.4  0.6  0.6 -0.6  2.   0.5]\n",
      " [ 0.3 -0.1 -2.4 -1.9  4.9  1.8]\n",
      " [-0.7 -0.7  0.4 -0.1 -4.5 -0.7]\n",
      " [-2.   3.3  2.1  1.6 -1.3  2.8]]\n",
      "Self-Attention Matrix:\n",
      " [[4.9e-07 4.0e-14 9.9e-01 1.7e-05 6.1e-03]\n",
      " [4.8e-01 3.6e-01 1.5e-01 2.1e-02 4.0e-04]\n",
      " [1.9e-02 5.8e-01 9.4e-02 2.8e-01 3.2e-02]\n",
      " [3.0e-02 1.2e-02 8.5e-01 9.9e-02 1.1e-02]\n",
      " [4.7e-04 7.3e-03 1.6e-02 4.0e-02 9.4e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the output\n",
    "output, attention_weights = self_attention(X, Wq, Wk, Wv)\n",
    "\n",
    "# Print in a nice format\n",
    "np.set_printoptions(precision=1)\n",
    "print(\"Self-Attention Output:\\n\", output)\n",
    "print(\"Self-Attention Matrix:\\n\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644c648-2f0a-4ae2-a50e-17d0c7f412bb",
   "metadata": {},
   "source": [
    "### (b) Implement multi-head attention, using the previously implemented function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e149e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, Wq, Wk, Wv, H):\n",
    "    # split the dimension D into H heads\n",
    "    head_idx = np.split(np.arange(Wk.shape[1]), H)\n",
    "    head_outputs = []\n",
    "    head_weights = []\n",
    "    for idx in head_idx:\n",
    "        output, weights = self_attention(X, Wq[:, idx], Wk[:, idx], Wv[:, idx])\n",
    "        head_outputs.append(output)\n",
    "        head_weights.append(weights)\n",
    "    output = np.concatenate(head_outputs, axis=1)\n",
    "\n",
    "    return output, head_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5563ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention Output:\n",
      " [[-0.7 -0.9  0.7  0.2 -1.5  2.9]\n",
      " [-1.1  0.9 -3.9 -2.9 -3.  -0.5]\n",
      " [-0.7 -0.9  1.6  1.2  5.8  1.5]\n",
      " [-0.7 -0.7 -3.8 -2.8 -5.1 -0.9]\n",
      " [-0.2  0.3  1.   0.2 -1.3  3. ]]\n",
      "Self-Attention Weights:\n",
      "Head 1\n",
      " [[1.8e-04 1.2e-03 9.5e-01 4.6e-02 2.3e-05]\n",
      " [5.1e-01 1.7e-02 3.6e-01 1.0e-02 1.0e-01]\n",
      " [6.4e-04 2.5e-03 9.4e-01 5.4e-02 9.7e-05]\n",
      " [3.8e-02 2.6e-02 8.4e-01 8.8e-02 1.0e-02]\n",
      " [2.6e-02 3.2e-01 4.5e-02 5.3e-01 7.7e-02]]\n",
      "\n",
      "Head 2\n",
      " [[3.1e-05 2.0e-18 9.0e-01 4.8e-07 9.7e-02]\n",
      " [6.1e-04 1.0e+00 9.6e-05 8.1e-04 9.0e-07]\n",
      " [3.2e-03 1.1e-01 3.8e-04 1.5e-02 8.7e-01]\n",
      " [1.0e-02 9.7e-01 2.3e-03 1.8e-02 2.0e-03]\n",
      " [1.1e-01 7.5e-04 7.9e-01 4.0e-02 5.4e-02]]\n",
      "\n",
      "Head 3\n",
      " [[3.1e-05 3.9e-05 1.8e-02 3.6e-03 9.8e-01]\n",
      " [4.3e-01 4.7e-03 4.9e-01 7.1e-02 6.8e-03]\n",
      " [2.3e-01 6.7e-01 2.2e-02 6.3e-02 1.5e-02]\n",
      " [1.4e-02 4.5e-05 9.1e-01 2.7e-02 4.4e-02]\n",
      " [2.8e-06 3.9e-03 1.0e-04 8.4e-04 1.0e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute multi-head attention\n",
    "H = 3\n",
    "attention_output = multi_head_attention(X, Wq, Wk, Wv, H)\n",
    "# Again print the requested results\n",
    "print(\"Self-Attention Output:\\n\", attention_output[0])\n",
    "print(\"Self-Attention Weights:\")\n",
    "for i, m in enumerate(attention_output[1]):\n",
    "    print(f\"Head {i+1}\\n {m}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c2abb-23bf-4772-9090-420aafac4639",
   "metadata": {},
   "source": [
    "### (c+d) Provide the answers/explanations requested in the problem sheet:\n",
    "1. Why the results are different?\n",
    "2. What happens if you change the order of two inputs="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca284a",
   "metadata": {},
   "source": [
    "For $H=1$ we have\n",
    "```\n",
    "Self-Attention Output:\n",
    " [[-0.7 -0.9  0.5  0.1 -5.5 -1.1]\n",
    " [-0.4  0.6  0.6 -0.6  2.   0.5]\n",
    " [ 0.3 -0.1 -2.4 -1.9  4.9  1.8]\n",
    " [-0.7 -0.7  0.4 -0.1 -4.5 -0.7]\n",
    " [-2.   3.3  2.1  1.6 -1.3  2.8]]\n",
    "Self-Attention Matrix:\n",
    " [[4.9e-07 4.0e-14 9.9e-01 1.7e-05 6.1e-03]\n",
    " [4.8e-01 3.6e-01 1.5e-01 2.1e-02 4.0e-04]\n",
    " [1.9e-02 5.8e-01 9.4e-02 2.8e-01 3.2e-02]\n",
    " [3.0e-02 1.2e-02 8.5e-01 9.9e-02 1.1e-02]\n",
    " [4.7e-04 7.3e-03 1.6e-02 4.0e-02 9.4e-01]]\n",
    "\n",
    "```\n",
    "\n",
    "For $H=3$ we have\n",
    "```\n",
    "Self-Attention Output:\n",
    " [[-0.7 -0.9  0.7  0.2 -1.5  2.9]\n",
    " [-1.1  0.9 -3.9 -2.9 -3.  -0.5]\n",
    " [-0.7 -0.9  1.6  1.2  5.8  1.5]\n",
    " [-0.7 -0.7 -3.8 -2.8 -5.1 -0.9]\n",
    " [-0.2  0.3  1.   0.2 -1.3  3. ]]\n",
    "Self-Attention Weights:\n",
    "Head 1\n",
    " [[1.8e-04 1.2e-03 9.5e-01 4.6e-02 2.3e-05]\n",
    " [5.1e-01 1.7e-02 3.6e-01 1.0e-02 1.0e-01]\n",
    " [6.4e-04 2.5e-03 9.4e-01 5.4e-02 9.7e-05]\n",
    " [3.8e-02 2.6e-02 8.4e-01 8.8e-02 1.0e-02]\n",
    " [2.6e-02 3.2e-01 4.5e-02 5.3e-01 7.7e-02]]\n",
    "\n",
    "Head 2\n",
    " [[3.1e-05 2.0e-18 9.0e-01 4.8e-07 9.7e-02]\n",
    " [6.1e-04 1.0e+00 9.6e-05 8.1e-04 9.0e-07]\n",
    " [3.2e-03 1.1e-01 3.8e-04 1.5e-02 8.7e-01]\n",
    " [1.0e-02 9.7e-01 2.3e-03 1.8e-02 2.0e-03]\n",
    " [1.1e-01 7.5e-04 7.9e-01 4.0e-02 5.4e-02]]\n",
    "\n",
    "Head 3\n",
    " [[3.1e-05 3.9e-05 1.8e-02 3.6e-03 9.8e-01]\n",
    " [4.3e-01 4.7e-03 4.9e-01 7.1e-02 6.8e-03]\n",
    " [2.3e-01 6.7e-01 2.2e-02 6.3e-02 1.5e-02]\n",
    " [1.4e-02 4.5e-05 9.1e-01 2.7e-02 4.4e-02]\n",
    " [2.8e-06 3.9e-03 1.0e-04 8.4e-04 1.0e+00]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93e9ea",
   "metadata": {},
   "source": [
    "The results are different because the shape of the attention matrix is $N \\times N$, where $N$ is the number of inputs, i.e. it is independent of the dimension $D$ of the input. In other words, if we compute the self-attention with $H$ heads, we get $H$ times $N \\times N$ attention matrices, instead of just a single attention matrix when using a single head. So the attention matrix is not \"sliced\" even though the other other matrices ($Q$, $K$, $V$) are, leading to a different result because the values $V$ are *routed* differently to create the output in the two cases. This leads to the multi-head self-attention being more expressive, since each head can pay attention to different characteristics of the input simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e7db2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      " [[ 0.7 -0.8 -1.2 -1.  -0.  -0.3]\n",
      " [ 2.7  0.1  1.6  1.8  1.5  0.3]\n",
      " [ 0.1  2.6 -0.1 -1.3 -0.5 -0.7]\n",
      " [ 1.1  1.5  1.  -0.5  0.4  0.4]\n",
      " [-0.7 -0.7  0.7 -1.5 -0.8  1. ]]\n",
      "X after permutation\n",
      " [[ 2.7  0.1  1.6  1.8  1.5  0.3]\n",
      " [ 0.7 -0.8 -1.2 -1.  -0.  -0.3]\n",
      " [ 0.1  2.6 -0.1 -1.3 -0.5 -0.7]\n",
      " [ 1.1  1.5  1.  -0.5  0.4  0.4]\n",
      " [-0.7 -0.7  0.7 -1.5 -0.8  1. ]]\n"
     ]
    }
   ],
   "source": [
    "# change the order of first two inputs\n",
    "print(\"X\\n\",X)\n",
    "X = X[[1,0,2,3,4],:]\n",
    "print(\"X after permutation\\n\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f43d9f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention Output:\n",
      " [[-0.4  0.6  0.6 -0.6  2.   0.5]\n",
      " [-0.7 -0.9  0.5  0.1 -5.5 -1.1]\n",
      " [ 0.3 -0.1 -2.4 -1.9  4.9  1.8]\n",
      " [-0.7 -0.7  0.4 -0.1 -4.5 -0.7]\n",
      " [-2.   3.3  2.1  1.6 -1.3  2.8]]\n",
      "Self-Attention Weights:\n",
      "Head 1\n",
      " [[3.6e-01 4.8e-01 1.5e-01 2.1e-02 4.0e-04]\n",
      " [4.0e-14 4.9e-07 9.9e-01 1.7e-05 6.1e-03]\n",
      " [5.8e-01 1.9e-02 9.4e-02 2.8e-01 3.2e-02]\n",
      " [1.2e-02 3.0e-02 8.5e-01 9.9e-02 1.1e-02]\n",
      " [7.3e-03 4.7e-04 1.6e-02 4.0e-02 9.4e-01]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "H = 1\n",
    "attention_output = multi_head_attention(X, Wq, Wk, Wv, H)\n",
    "# Again print the requested results\n",
    "print(\"Self-Attention Output:\\n\", attention_output[0])\n",
    "print(\"Self-Attention Weights:\")\n",
    "for i, m in enumerate(attention_output[1]):\n",
    "    print(f\"Head {i+1}\\n {m}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ed844ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention Output:\n",
      " [[-1.1  0.9 -3.9 -2.9 -3.  -0.5]\n",
      " [-0.7 -0.9  0.7  0.2 -1.5  2.9]\n",
      " [-0.7 -0.9  1.6  1.2  5.8  1.5]\n",
      " [-0.7 -0.7 -3.8 -2.8 -5.1 -0.9]\n",
      " [-0.2  0.3  1.   0.2 -1.3  3. ]]\n",
      "Self-Attention Weights:\n",
      "Head 1\n",
      " [[1.7e-02 5.1e-01 3.6e-01 1.0e-02 1.0e-01]\n",
      " [1.2e-03 1.8e-04 9.5e-01 4.6e-02 2.3e-05]\n",
      " [2.5e-03 6.4e-04 9.4e-01 5.4e-02 9.7e-05]\n",
      " [2.6e-02 3.8e-02 8.4e-01 8.8e-02 1.0e-02]\n",
      " [3.2e-01 2.6e-02 4.5e-02 5.3e-01 7.7e-02]]\n",
      "\n",
      "Head 2\n",
      " [[1.0e+00 6.1e-04 9.6e-05 8.1e-04 9.0e-07]\n",
      " [2.0e-18 3.1e-05 9.0e-01 4.8e-07 9.7e-02]\n",
      " [1.1e-01 3.2e-03 3.8e-04 1.5e-02 8.7e-01]\n",
      " [9.7e-01 1.0e-02 2.3e-03 1.8e-02 2.0e-03]\n",
      " [7.5e-04 1.1e-01 7.9e-01 4.0e-02 5.4e-02]]\n",
      "\n",
      "Head 3\n",
      " [[4.7e-03 4.3e-01 4.9e-01 7.1e-02 6.8e-03]\n",
      " [3.9e-05 3.1e-05 1.8e-02 3.6e-03 9.8e-01]\n",
      " [6.7e-01 2.3e-01 2.2e-02 6.3e-02 1.5e-02]\n",
      " [4.5e-05 1.4e-02 9.1e-01 2.7e-02 4.4e-02]\n",
      " [3.9e-03 2.8e-06 1.0e-04 8.4e-04 1.0e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "H = 3\n",
    "attention_output = multi_head_attention(X, Wq, Wk, Wv, H)\n",
    "# Again print the requested results\n",
    "print(\"Self-Attention Output:\\n\", attention_output[0])\n",
    "print(\"Self-Attention Weights:\")\n",
    "for i, m in enumerate(attention_output[1]):\n",
    "    print(f\"Head {i+1}\\n {m}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ff033c",
   "metadata": {},
   "source": [
    "The output of the self-attention is thus *equivariant* to permutations of the input $X$. However, the attention weights need to be different for this to be true. It takes as input a set, paying no attention to ordering. The ordering can be taken into account by positional encoding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
